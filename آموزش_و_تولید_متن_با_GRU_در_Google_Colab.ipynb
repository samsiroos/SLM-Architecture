{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samsiroos/SLM-Architecture/blob/main/%D8%A2%D9%85%D9%88%D8%B2%D8%B4_%D9%88_%D8%AA%D9%88%D9%84%DB%8C%D8%AF_%D9%85%D8%AA%D9%86_%D8%A8%D8%A7_GRU_%D8%AF%D8%B1_Google_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import os # For file system operations\n",
        "\n",
        "print(\"Libraries successfully imported.\")\n",
        "\n",
        "# --- 1. Prepare the Training Data File ---\n",
        "# Define the filename for the training data and the model\n",
        "training_data_file = \"training_data.txt\"\n",
        "model_filename = \"my_gru_model.keras\" # Using .keras extension for newer TensorFlow versions\n",
        "\n",
        "# NOTE: The 'initial_training_text' variable has been removed as per your request.\n",
        "# Please ensure that 'training_data.txt' exists in your Colab environment\n",
        "# with the desired training content before running this script,\n",
        "# especially if you are starting a new Colab session or the file was deleted.\n",
        "# You can manually upload a 'training_data.txt' file or create it using Colab's file system.\n",
        "\n",
        "# Read the text from the file\n",
        "try:\n",
        "    with open(training_data_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "    print(f\"Text successfully loaded from '{training_data_file}'.\")\n",
        "    if not text.strip(): # Check if the file is empty after stripping whitespace\n",
        "        print(f\"Warning: The file '{training_data_file}' is empty. Please add content to it for training.\")\n",
        "        exit() # Exit if the file is empty\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{training_data_file}' was not found. Please ensure it's in the correct path and contains training data.\")\n",
        "    print(\"You need to create this file manually or upload it to your Colab environment.\")\n",
        "    exit() # Exit if the file is not found\n",
        "\n",
        "# Convert all text to lowercase for consistency\n",
        "text = text.lower()\n",
        "print(f\"\\nOriginal text for training (first part):\\n{text[:500]}...\\n\") # Display a portion of the text\n",
        "\n",
        "# --- Tokenization ---\n",
        "# Tokenization is the process of converting text into a sequence of numbers.\n",
        "# Each unique word receives a unique index.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text]) # Train the tokenizer on the text to recognize words\n",
        "word_index = tokenizer.word_index # Dictionary of words to their indices\n",
        "print(f\"Number of unique words in the vocabulary: {len(word_index)}\\n\")\n",
        "\n",
        "# Total number of unique words + 1 (for words not found in the vocabulary or for padding)\n",
        "total_words = len(word_index) + 1\n",
        "print(f\"Total unique words in vocabulary (including zero for padding): {total_words}\\n\")\n",
        "\n",
        "# --- Create input and output sequences for training ---\n",
        "# For training the model, we create sequences of words (n-grams).\n",
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    if line.strip() == \"\": # Skip empty lines\n",
        "        continue\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0] # Convert each line to a sequence of numbers\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1] # n-gram sequence (from the beginning of the line up to the current word)\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "print(f\"Number of input sequences for training: {len(input_sequences)}\\n\")\n",
        "print(f\"Example input sequences (numbers):\\n{input_sequences[:5]}\\n\")\n",
        "\n",
        "# --- Pad sequences and separate X and Y ---\n",
        "# Neural networks require inputs to be of uniform size.\n",
        "# Therefore, shorter sequences are padded with zeros at the beginning (pre-padding).\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "print(f\"Maximum sequence length in training data: {max_sequence_len}\\n\")\n",
        "\n",
        "padded_sequences = np.array(tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                          maxlen=max_sequence_len,\n",
        "                                                                          padding='pre'))\n",
        "\n",
        "# X (input): All words in the sequence except the last word\n",
        "X = padded_sequences[:, :-1]\n",
        "# labels (output): Only the last word in each sequence (the word we want to predict)\n",
        "labels = padded_sequences[:, -1]\n",
        "\n",
        "# Convert output (labels) to one-hot encoding format\n",
        "# Since word prediction is a classification problem, outputs must be one-hot.\n",
        "y = to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "print(f\"Input dimensions (X) for the model: {X.shape}\") # (Number of samples, input sequence length)\n",
        "print(f\"Output dimensions (y) for the model (one-hot): {y.shape}\") # (Number of samples, total number of words in vocabulary)\n",
        "\n",
        "# --- Check for existing model and handle training/loading ---\n",
        "model = None\n",
        "train_model = True # Default to train if no model exists or user wants to retrain\n",
        "\n",
        "if os.path.exists(model_filename):\n",
        "    print(f\"\\nFound existing model: '{model_filename}'.\")\n",
        "    # Using a simple input for user choice. In a real app, you'd use a UI element.\n",
        "    user_choice = input(\"Do you want to retrain the model? (yes/no): \").lower()\n",
        "    if user_choice == 'no':\n",
        "        train_model = False\n",
        "        try:\n",
        "            model = load_model(model_filename)\n",
        "            print(f\"Model successfully loaded from '{model_filename}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}. Will proceed with training a new model.\")\n",
        "            train_model = True # Fallback to training if loading fails\n",
        "    else:\n",
        "        print(\"Retraining requested by user.\")\n",
        "else:\n",
        "    print(f\"\\nNo existing model found at '{model_filename}'. A new model will be trained.\")\n",
        "\n",
        "if train_model:\n",
        "    # --- 2. Build the GRU Model ---\n",
        "    # Define the GRU model architecture with updated parameters.\n",
        "    model = Sequential()\n",
        "    # Embedding layer: Increase word vector dimension to 256.\n",
        "    # total_words: vocabulary size\n",
        "    # 256: word vector dimension (embedding_dim)\n",
        "    # input_length: input sequence length for the GRU layer (max_sequence_len - 1)\n",
        "    model.add(Embedding(total_words, 256, input_length=max_sequence_len-1))\n",
        "    # First GRU layer: 512 units.\n",
        "    # return_sequences=True: Passes output of each timestep to the next GRU layer.\n",
        "    model.add(GRU(512, return_sequences=True))\n",
        "    # Second GRU layer: 512 units.\n",
        "    # return_sequences=False: Passes only the output of the last timestep to the Dense layer.\n",
        "    model.add(GRU(512, return_sequences=False))\n",
        "    # Dense layer: Output layer with softmax activation.\n",
        "    # total_words: number of neurons equals the total number of words in the vocabulary.\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    # Compile the model: define loss function, optimizer, and evaluation metrics\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # --- 3. Train the Model ---\n",
        "    # Increase the number of epochs to 200.\n",
        "    # This step can be time-consuming, especially without GPU.\n",
        "    print(\"\\nStarting GRU model training...\")\n",
        "    history = model.fit(X, y, epochs=200, verbose=1) # verbose=1 to display training details\n",
        "\n",
        "    print(\"\\nModel training finished.\")\n",
        "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "    print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save(model_filename)\n",
        "    print(f\"Model successfully saved to '{model_filename}'.\")\n",
        "else:\n",
        "    # If the model was loaded, display its summary\n",
        "    if model:\n",
        "        print(\"\\nUsing the pre-existing model.\")\n",
        "        model.summary()\n",
        "    else:\n",
        "        print(\"Error: Model could not be loaded or built. Please check the setup.\")\n",
        "        exit()\n",
        "\n",
        "# --- 4. Generate Text with the Trained Model ---\n",
        "# Function to generate next words based on a seed text\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len, tokenizer):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(next_words):\n",
        "        # Convert seed text to numerical sequence\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "        # Pad the sequence for model input\n",
        "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list],\n",
        "                                                                  maxlen=max_sequence_len-1,\n",
        "                                                                  padding='pre')\n",
        "\n",
        "        # Predict the probability of each word as the next word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        # Select the word with the highest probability (argmax)\n",
        "        predicted_word_index = np.argmax(predicted_probs)\n",
        "\n",
        "        output_word = \"\"\n",
        "        # Find the word corresponding to the predicted index\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_word_index:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # If no word is found (e.g., index 0 for padding)\n",
        "        if output_word == \"\":\n",
        "            output_word = \"<unk>\" # Unknown word placeholder\n",
        "\n",
        "        generated_text += \" \" + output_word\n",
        "    return generated_text\n",
        "\n",
        "# --- Interactive Text Generation ---\n",
        "print(\"\\n--- Interactive Text Generation ---\")\n",
        "continue_generating = 'yes'\n",
        "while continue_generating.lower() == 'yes':\n",
        "    seed_text_input = input(\"\\nلطفاً یک عبارت اولیه برای تولید متن وارد کنید (به فارسی): \")\n",
        "    num_words_to_generate = 20 # تعداد کلماتی که می‌خواهید مدل تولید کند\n",
        "\n",
        "    print(f\"\\nشروع تولید متن با عبارت اولیه: '{seed_text_input}'\")\n",
        "    generated_sentence = generate_text(seed_text_input.lower(), num_words_to_generate, model, max_sequence_len, tokenizer)\n",
        "    print(f\"\\nمتن تولید شده توسط مدل:\\n{generated_sentence}\")\n",
        "\n",
        "    continue_generating = input(\"\\nآیا می‌خواهید جمله جدیدی بسازید؟ (بله/خیر): \")\n",
        "    if continue_generating.lower() == 'بله':\n",
        "        continue_generating = 'yes'\n",
        "    else:\n",
        "        continue_generating = 'no'\n",
        "\n",
        "print(\"\\nپایان تولید متن.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries successfully imported.\n",
            "Text successfully loaded from 'training_data.txt'.\n",
            "\n",
            "Original text for training (first part):\n",
            "\n",
            "جهان هستی پر از شگفتی‌هاست. هر ستاره در آسمان شب، داستانی ناگفته دارد. کهکشان‌ها با میلیاردها ستاره، منظومه‌های شمسی بی‌شمار را در خود جای داده‌اند. زمین، سیاره آبی ما، تنها گوشه‌ای از این عظمت بی‌کران است. حیات در آن به شکل‌های گوناگون جریان دارد. از کوچکترین میکروارگانیسم‌ها تا بزرگترین نهنگ‌ها. همه و همه بخشی از این چرخه شگفت‌انگیز هستند.\n",
            "\n",
            "علم به ما کمک می‌کند تا این رازها را کشف کنیم. فیزیک، شیمی، زیست‌شناسی، و نجوم. هر کدام دریچه‌ای نو به سوی درک عمیق‌تر جهان می‌گشایند. اکتشافات علمی، مرزه...\n",
            "\n",
            "Number of unique words in the vocabulary: 292\n",
            "\n",
            "Total unique words in vocabulary (including zero for padding): 293\n",
            "\n",
            "Number of input sequences for training: 1848\n",
            "\n",
            "Example input sequences (numbers):\n",
            "[[26, 61], [26, 61, 62], [26, 61, 62, 2], [26, 61, 62, 2, 63], [26, 61, 62, 2, 63, 12]]\n",
            "\n",
            "Maximum sequence length in training data: 59\n",
            "\n",
            "Input dimensions (X) for the model: (1848, 58)\n",
            "Output dimensions (y) for the model (one-hot): (1848, 293)\n",
            "\n",
            "Found existing model: 'my_gru_model.keras'.\n",
            "Do you want to retrain the model? (yes/no): no\n",
            "Model successfully loaded from 'my_gru_model.keras'.\n",
            "\n",
            "Using the pre-existing model.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m75,008\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_6 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,182,720\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_7 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,575,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m293\u001b[0m)            │       \u001b[38;5;34m150,309\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">75,008</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,720</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">293</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">150,309</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,951,921\u001b[0m (34.15 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,951,921</span> (34.15 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,983,973\u001b[0m (11.38 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,983,973</span> (11.38 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m5,967,948\u001b[0m (22.77 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,967,948</span> (22.77 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Interactive Text Generation ---\n",
            "\n",
            "لطفاً یک عبارت اولیه برای تولید متن وارد کنید (به فارسی): میکروارگانیسم‌ها \n",
            "\n",
            "شروع تولید متن با عبارت اولیه: 'میکروارگانیسم‌ها '\n",
            "\n",
            "متن تولید شده توسط مدل:\n",
            "میکروارگانیسم‌ها  به ما کمک می‌کند تا این رازها را کشف کنیم فیزیک، شیمی، زیست‌شناسی، و نجوم هر کدام دریچه‌ای نو به\n",
            "\n",
            "آیا می‌خواهید جمله جدیدی بسازید؟ (بله/خیر): بله\n",
            "\n",
            "لطفاً یک عبارت اولیه برای تولید متن وارد کنید (به فارسی): فیزیک\n",
            "\n",
            "شروع تولید متن با عبارت اولیه: 'فیزیک'\n",
            "\n",
            "متن تولید شده توسط مدل:\n",
            "فیزیک زیست ما در خطر است تغییرات اقلیمی، آلودگی هوا و آب از بین رفتن تنوع زیستی این‌ها چالش‌های بزرگی هستند\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eABhYkTv5TqX",
        "outputId": "6fc71960-3432-47e5-cf73-8aa6cd4c26b3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}